{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from textblob import WordList, TextBlob\n",
    "import re\n",
    "import codecs\n",
    "from datetime import datetime\n",
    "import time\n",
    "import string\n",
    "from collections import Counter\n",
    "from nltk import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"~/Desktop/FOIA/master-1-11.csv\", header = 0, low_memory = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLEAN AND EXTRACT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "del data['Unnamed: 0'] \n",
    "data = data[data['closed_date'] != 'TBD']\n",
    "data = data[data['description'] != '']\n",
    "data = data[data['description'] != 'The description of this request is under Agency review.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def remove_non_ascii(text):\n",
    "    return ''.join([i if ord(i) < 128 else '' for i in text])\n",
    "\n",
    "data[\"request\"] = data['description'].str.apply(remove_non_ascii)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def del_whites(text):\n",
    "    text = re.sub(r'\\s', ' ', text).strip()\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "data[\"request\"] = data['request'].apply(del_whites)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def cut_greeting(text):\n",
    "    match = re.search('(|.+?)(Mrs?\\..+?[,:]|Ms\\..+?[,:]|Miss.+?[,:]|Mister.+?[,:]|[Mm]adams?[,:]|[Ss]irs?[,:]|To [Ww]hom [Ii]t [Mm]ay [Cc]oncern[,:]|Dear.+?[,:])(.+)', text)\n",
    "    if match:\n",
    "        return match.groups(0)[2]\n",
    "    else:\n",
    "        return text\n",
    "        \n",
    "data['request_no_greeting'] = data['request'].apply(cut_greeting)\n",
    "data['request_no_greeting'] = data['request_no_greeting'].str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def cut_salutation(text):\n",
    "    match = re.search('(.+)(Sincerely,|[Bb]est,|From,|[Tt]hanks,|Best [Ww]ishes,|[Rr]egards,|Thank [Yy]ou,|My best to you,|Warmly,|Take care,|Thanks so much,|Thank you,|Thanks for your consideration,|Looking forward,|Be well,)(.*)', text)\n",
    "    if match:\n",
    "        return match.groups(0)[0]\n",
    "    else:\n",
    "        return text\n",
    "    \n",
    "data['request_body'] = data['request_no_greeting'].apply(cut_salutation)\n",
    "data['request_body'] = data['request_body'].str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data['body_lowercase'] = data['request_body'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def charify(text):\n",
    "    exclude = set(string.punctuation)\n",
    "    text = ''.join(char for char in text if char not in exclude)\n",
    "    text = re.sub('\\s+', '', text).strip()\n",
    "    return len(text)\n",
    "\n",
    "data['char_count'] = data['body_lowercase'].apply(charify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def wordify(text):\n",
    "    text = TextBlob(text)\n",
    "    list_words = text.words\n",
    "    return len(list_words)\n",
    "\n",
    "data['word_count'] = data['body_lowercase'].apply(wordify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sentify(text):\n",
    "    text = TextBlob(text)\n",
    "    list_sens = text.sentences\n",
    "    return len(list_sens)\n",
    "\n",
    "data['sen_count'] = data['body_lowercase'].apply(sentify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data['avg_sen_len'] = data['word_count']/data['sen_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def to_date(x):\n",
    "    t = datetime.strptime(x, \"%a %b %d %H %M %S %Z %Y\")\n",
    "    return t\n",
    "\n",
    "data[\"closed_datetime\"] = data[\"closed_date\"].apply(to_date)\n",
    "data[\"submitted_datetime\"] = data[\"date_submitted\"].apply(to_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def to_seconds(x):\n",
    "    return time.mktime(x.timetuple())\n",
    "\n",
    "data['duration'] = (data[\"closed_datetime\"].apply(to_seconds) - data[\"submitted_datetime\"].apply(to_seconds)) / 86400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def foia(text):\n",
    "    if ('freedom of information' in text) or ('foia' in text):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "data[\"ref_foia\"] = data['body_lowercase'].apply(foia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fees(text):\n",
    "    match = re.search('[.,\\/#!$%\\^&\\*;:{}=\\-_`~()\\s]fees?[.,\\/#!$%\\^&\\*;:{}=\\-_`~()\\s]', text)\n",
    "    if match:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "data[\"ref_fees\"] = data['body_lowercase'].apply(fees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def agency(tracking_number):\n",
    "    match = re.search('(.*?)(-.*)', tracking_number)\n",
    "    if match:\n",
    "        return match.groups(0)[0]\n",
    "\n",
    "data[\"_agency\"] = data['tracking number'].apply(agency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def phone_number(text):\n",
    "    match = re.search('(\\d{3}[-\\.\\s]??\\d{3}[-\\.\\s]??\\d{4}|\\(\\d{3}\\)\\s*\\d{3}[-\\.\\s]??\\d{4}|\\d{3}[-\\.\\s]??\\d{4})', text)\n",
    "    if match:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "data[\"phone_number\"] = data['request_no_greeting'].apply(phone_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hyperlink(text):\n",
    "    match = re.search('((https?):((//)|(\\\\\\\\))+[\\w\\d:#@%/;$()~_?\\+-=\\\\\\.&]*)', text)\n",
    "    match2 = re.search('(www.(?:[a-z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+)', text)\n",
    "    if match or match2:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "data[\"hyperlink\"] = data['body_lowercase'].apply(hyperlink)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def email_address(text):\n",
    "    match = re.search('([\\w\\-\\.]+@(\\w[\\w\\-]+\\.)+[\\w\\-]+)', text)\n",
    "    if match:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "data[\"email_address\"] = data['request_no_greeting'].apply(email_address)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def date(text):\n",
    "    match = re.search('(\\d{1,2}[\\.\\-\\/]\\d{1,2}[\\.\\-\\/]\\d{2,4}|[Jj]anuary|[Ff]ebruary|[Mm]arch|[Aa]pril|[Mm]ay|[Jj]une|[Jj]uly|[Aa]ugust|[Ss]eptember|[Nn]ovember|[Dd]ecember|[Mm]on\\b|[Tt]ues\\b|[Ww]ed\\b|[Tt]hurs\\b|[Ff]ri\\b|[S]at\\b|[Ss]un\\b|[[Jj]an\\b|[Ff]eb\\b|[Mm]ar\\b|[Aa]pr\\b|[Aa]ug\\b|[Ss]ept\\b|[Oo]ct\\b|[Nn]ov\\b|[Dd]ec[Mm]onday|[Tt]uesday|[Ww]ednesday|[Tt]hursday|[Ff]riday|[Ss]aturday|[Ss]unday)', text)\n",
    "    match2 = re.search('( )(19|20)(\\d\\d)( |-19\\d\\d|-20\\d\\d|-\\d\\d)', text)\n",
    "    if match or match2:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "data[\"ref_date\"] = data['request_body'].apply(date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "us_states = pd.read_csv(\"~/Desktop/FOIA/states.csv\", header = 0)\n",
    "\n",
    "states = us_states['State']\n",
    "states_lower = us_states['State'].str.lower()\n",
    "abbrev = us_states['Abbreviation']\n",
    "postal = us_states['Postal']\n",
    "postal_lower = us_states['Postal'].str.lower()\n",
    "\n",
    "def ref_place(text):\n",
    "    if any(substring in text for substring in states):\n",
    "        return 1\n",
    "    elif any(substring in text for substring in states_lower):\n",
    "        return 1\n",
    "    elif any(substring in text for substring in abbrev):\n",
    "        return 1\n",
    "    elif any(substring in text for substring in postal):\n",
    "        return 1\n",
    "    elif any(substring in text for substring in postal_lower):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    " \n",
    "data[\"ref_place\"] = data['request_body'].apply(ref_place)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https://en.wikipedia.org/wiki/Automated_readability_index\n",
    "# Note actual intellgience paper\n",
    "\n",
    "data['readability'] = 4.71*(data['char_count']/data['word_count']) + 0.5*(data['word_count']/data['sen_count']) - 21.43\n",
    "\n",
    "def max_out(x):\n",
    "    if x > 20:\n",
    "        return 20\n",
    "    else:\n",
    "        return x\n",
    "    \n",
    "data_filtered['readability'] = data_filtered['readability'].apply(max_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mention_data(text):\n",
    "    data_terms = ['affymetrix',\n",
    "    'aiff',\n",
    "    'apache',\n",
    "    'arcgrid',\n",
    "    'bdf',\n",
    "    'binary',\n",
    "    'bmp',\n",
    "    'byte',\n",
    "    'byu',\n",
    "    'bzip',\n",
    "    'cded',\n",
    "    'cdf',\n",
    "    'column',\n",
    "    'csv',\n",
    "    'data',\n",
    "    'delimited',\n",
    "    'directory',\n",
    "    'dx',\n",
    "    'dxf',\n",
    "    'edf',\n",
    "    'emf',\n",
    "    'eml',\n",
    "    'eps',\n",
    "    'excel',\n",
    "    'exr',\n",
    "    'fastq',\n",
    "    'flac',\n",
    "    'flv',\n",
    "    'fq',\n",
    "    'fsa',\n",
    "    'gbk',\n",
    "    'genbank',\n",
    "    'geotiff',\n",
    "    'gml',\n",
    "    'gpx',\n",
    "    'graphlet',\n",
    "    'grb',\n",
    "    'grd',\n",
    "    'grib',\n",
    "    'gtopo30',\n",
    "    'gw',\n",
    "    'gxl',\n",
    "    'gz',\n",
    "    'gzip',\n",
    "    'harwellboeing',\n",
    "    'hdf',\n",
    "    'hdf5',\n",
    "    'j2k',\n",
    "    'jcm',\n",
    "    'jdx',\n",
    "    'jp2',\n",
    "    'json',\n",
    "    'jvx',\n",
    "    'kml',\n",
    "    'kmz',\n",
    "    'latex',\n",
    "    'lgr',\n",
    "    'lwo',\n",
    "    'mbox',\n",
    "    'mdb',\n",
    "    'mgf',\n",
    "    'mol2',\n",
    "    'mpfa',\n",
    "    'mps',\n",
    "    'mtp',\n",
    "    'mtx',\n",
    "    'mx',\n",
    "    'nb',\n",
    "    'ndk',\n",
    "    'netcdf',\n",
    "    'noff',\n",
    "    'nxs',\n",
    "    'obj',\n",
    "    'pbm',\n",
    "    'pcx',\n",
    "    'pdb',\n",
    "    'pgm',\n",
    "    'pnm',\n",
    "    'ppm',\n",
    "    'pxr',\n",
    "    'row',\n",
    "    'rss',\n",
    "    'sct',\n",
    "    'sdf',\n",
    "    'sff',\n",
    "    'shp',\n",
    "    'sp3',\n",
    "    'sql',\n",
    "    'structured query language',\n",
    "    'stx',\n",
    "    'svg',\n",
    "    'swf',\n",
    "    'sxc',\n",
    "    'table',\n",
    "    'tabular',\n",
    "    'tce',\n",
    "    'tga',\n",
    "    'tgf',\n",
    "    'tle',\n",
    "    'tsv',\n",
    "    'uue',\n",
    "    'vcf',\n",
    "    'vcs',\n",
    "    'vrml',\n",
    "    'vtk',\n",
    "    'webp',\n",
    "    'wmf',\n",
    "    'x3d',\n",
    "    'xbm',\n",
    "    'xls',\n",
    "    'xml',\n",
    "    'xport',\n",
    "    'xpt',\n",
    "    'xyz',\n",
    "    'zpr']\n",
    "    if any(substring in text for substring in data_terms):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "data['ref_data'] = data['body_lowercase'].apply(mention_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def specificity_count(text):\n",
    "\n",
    "    edited = str(text)\n",
    "\n",
    "    bag_of_words = word_tokenize(edited)\n",
    "    pos_tags = pos_tag(bag_of_words)\n",
    "    \n",
    "    counter = 0\n",
    "    last_tag = None \n",
    "    \n",
    "    for tagged_tuple in pos_tags:\n",
    "        if (tagged_tuple[1]=='NNP' and last_tag!='NNP'):\n",
    "            counter += 1 \n",
    "        last_tag = tagged_tuple[1]\n",
    "\n",
    "    return counter\n",
    "\n",
    "data['specificity'] = data['request_body'].apply(specificity_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def lemmatizer(text):\n",
    "    stem_list = []\n",
    "    text = TextBlob(text)\n",
    "    for word in text.words:\n",
    "        word = TextBlob(word)\n",
    "        POS = word.tags[0][1]\n",
    "        if POS.startswith(\"N\"):\n",
    "            stem = wnl.lemmatize(word, pos = 'n')\n",
    "            stem_list.append(str(stem))\n",
    "        elif POS.startswith(\"V\"):\n",
    "            stem = wnl.lemmatize(word, pos = 'v')\n",
    "            stem_list.append(str(stem))\n",
    "        else:\n",
    "            stem_list.append(str(word))\n",
    "    return ' '.join(stem_list)\n",
    "\n",
    "data['lemmatized_body'] = data['body_lowercase'].apply(lemmatizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def biclassify_strict(disposition):\n",
    "  if disposition == 'Full grant':\n",
    "    return 1\n",
    "  else:\n",
    "    return 0\n",
    "\n",
    "data['bi_strict'] = data['final_disposition'].apply(biclassify_strict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def biclassify_generous(disposition):\n",
    "  if (disposition == 'Full grant') or (disposition == 'Partial grant/partial denial'):\n",
    "    return 1\n",
    "  else:\n",
    "    return 0\n",
    "\n",
    "data['bi_gen'] = data['final_disposition'].apply(biclassify_generous)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def id(text):\n",
    "    match = re.search('((ID|No[.\\s]|Number|#).{,5}-\\d{4})|([A-Z]{2,}\\d{6,})|((ID|No[.\\s]|Number|#|[Ff]orm).{,5}\\d{4}-)|(API.*\\d{6,})|((ID|No[.\\s]|Number|#).{,5}\\d{3}-\\d{3})', text)\n",
    "    if match:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "data[\"id\"] = data['request_body'].apply(id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.to_csv('~/Desktop/FOIA/rich_foia_master.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
